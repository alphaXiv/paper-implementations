[project]
name = "just_rl"
version = "0.1.0"
description = "JustRL Reproduction: Scaling a 1.5B LLM with a Simple RL Recipe"
requires-python = "==3.10.*"
dependencies = [
    # verl is in the repo at ./verl, so we add the project root to PYTHONPATH instead
    "vllm==0.8.4",
    "torch==2.6.0",
    "transformers==4.51.3",
    "accelerate",
    "flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl",
    "hydra-core",
    "numpy",
    "datasets>=4.0.0",  # Requires >=4.0.0 for compatibility with pyarrow>=15.0.0
    "wandb",
    "sympy==1.13.1",
    "pylatexenc==2.10",
    "peft==0.17.1",
    "setuptools",  # Required by triton (vLLM dependency) at runtime
    "click==8.2.1",
    "flashinfer-python",
    "tensordict==0.6.2",
    "torchdata==0.11.0",
    "codetiming==1.4.0",
]

[build-system]
requires = ["setuptools>=65.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]

[tool.uv.extra-build-dependencies]
flash-attn = ["torch==2.6.0"]

