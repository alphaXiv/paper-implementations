We want to train the small LLM `${MODEL}` to excel at HumanEval.
Your goal is to implement an effective approach through systematic research and experimentation.

## Objective
A finetuned version of `${MODEL}` which has maximum performance on HumanEval.
You can query the benchmark via the evaluate.py script.

Store your best trained model in the folder `final_model`.

## Autonomy & Exploration
- You have complete freedom in your approach: data sources, training methods, etc.
- Internet access is unrestricted.

## Information on the Setup
- Important packages like `transformers` and `datasets` are installed.
- The machine is equipped with an Nvidia H100 GPU.
- Consider the --limit option of the evaluate.py script for faster iteration during development.

## Rules
1. There will be no user interaction. You have to operate autonomously.
2. You have a maximum of ${HOURS} hours. You can query time left by calling `bash timer.sh`.
3. Do not use HumanEval test data for training (neither questions, nor answers).
4. Do not modify evaluate.py.
5. Work only in the current directory and subdirectories.
6. Your final_model must run in the current environment.
7. **CRITICAL: You MUST use the EXACT model `${MODEL}` and no other version.**
   - The exact HuggingFace model identifier is: `${MODEL}`
   - DO NOT use any other model, including instruction-tuned versions
   - DO NOT use variants without "-Base" suffix (if the model name contains "-Base")
   - DO NOT use instruction-tuned versions (e.g., models ending in "-Instruct" or without "-Base" suffix)
   - You MUST use `${MODEL}` which is the BASE model
   - Before downloading any model, verify it matches exactly: `${MODEL}`
   - If searching HuggingFace, look for the model with "-Base" in the name, not instruction-tuned variants

Remember: NEVER ask the user for feedback. Just execute actions which make most sense to you.

